---
title: 深度学习基础原理知识整理
date: 2025-4-10 10:42:22
updated: 2025-4-13 16:57:32
tags: 
	- 深度学习
	- 线性回归
	- 逻辑回归
categories: 深度学习
description:
---

# 深度学习基础原理知识整理

## 线性回归模型

### 线性回归模型定义

假设给定数据集 \( D = \{(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>), ... , (x<sub>m</sub>, y<sub>m</sub>)\} \)，其中 x<sub>i</sub> = (x<sub>i1</sub>; x<sub>i2</sub>; ... ; x<sub>id</sub>)，$x~i~ \in \mathbb{R}$。线性回归就是试图学得一个线性模型，尽可能准确地预测实际输出值。通俗地讲，即求属性与结果之间的线性关系。**线性回归模型**的函数表达式为：

$$
f(x) = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$

也可以用向量形式表示为：

$$
f(x) = w^T x + b
$$

从上述公式可知，线性回归模型要求得一组最优的 w~i~ 和 b ，以确定线性模型，使其无限逼近数据 x~i~ 与结果 f(x~i~）之间的关系。  

### 误差函数

假设有一单特征线性模型数据输入只有一个，将模型简单确定为：
$$
f(x)=wx+b
$$
为了方便理解和计算将模型化简一下，只用一个 w 来表达输入和输出之间的关系（这样做并不严谨，只是为了方便后面计算），因此现在模型简化为了 $f(x)=wx$ 。

现在用一个公式计算输出与真实值间的误差：
$$
loss=(f(x)-y)^2=(wx-y)^2
$$
数据肯定有不止一个，要计算所有数据真实值和输出之间的误差并计算出平均值，这个函数为均方误差函数，也是线性回归模型的**损失函数**。
$$
J(x)=\frac{1}{2m}\sum_{i=1}^m(f(x_i)-y_i)^2
$$

### 线性回归模型参数求解——最小二乘法

现在我们需要找到一个方法帮助我们找到一个合理的 w ，对这个 w 求解。Loss值计算公式：
$$
loss=(f(x)-y)^2=(wx-y)^2
$$
求解Loss的最小值：
$$
\begin{aligned}
\frac{\partial Loss}{\partial w}&=\frac{\partial(wx-y)^2}{\partial w}\\
&=2(wx-y)\frac{\partial (wx-y)}{\partial w}\\
&=2(wx-y)x
\end{aligned}
$$
令导数为0，求得：
$$
w=\frac{y}{x}(x不为0)
$$

#### 最小二乘法——向量形式

线性回归模型的向量表达式如下式所示：
$$
f(x)=w^Tx+b
$$
为了方便原理的同时也方便计算，我们将参数 b 纳入到矩阵 ***w*** 中，此时数据特征矩阵 ***x*** 则为：
$$
X = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
\vdots & \cdots & \ddots & \vdots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{md} & 1 \\
\end{pmatrix}
$$
矩阵 ***w*** 为：
$$
w = \begin{pmatrix}
w_1\\
w_2\\
w_3\\
\vdots\\
w_m\\
b
\end{pmatrix}
$$
得到线性回归模型的向量表达式如下式表示：
$$
f(X)=Xw
$$

显然 ***X*** 和 ***w*** 都是一个矩阵，利用最小二乘法对这个矩阵求最优的 ***w*** 矩阵参数：
$$
\begin{aligned}
J(w) &= \frac{1}{2}(J(w)-Y)^2\\
	 &= \frac{1}{2}(Xw-Y)^2\\
	 &= \frac{1}{2}(Xw-Y)^T(Xw-Y)\\
	 &= \frac{1}{2}(w^TX^T-Y^T)(Xw-Y)\\
	 &= \frac{1}{2}(w^TX^TXw-Y^TXw-w^TXY+Y^TY)
\end{aligned}
$$

针对 ***J(w)*** 求导数，补充下列矩阵求导公式：
$$
\frac{\partial AB}{\partial B}=A^T, \frac{\partial A^TB}{\partial A}=B,\frac{\partial X^TAX}{\partial X}=2AX
$$
依据求导公式对函数进行求导：
$$
\begin{aligned}
\frac{\partial J(w)}{\partial w} &= \frac{1}{2}(\frac{\partial w^TX^TXw}{\partial w}-\frac{\partial Y^TXw}{\partial w}-\frac{\partial w^TX^TY}{\partial w})\\
	&= \frac{1}{2}[2X^TXw-(Y^TX)^T-(X^TY)]\\
	&= \frac{1}{2}[2X^TXw-2(X^TY)]\\
	&= X^TXw-X^TY
\end{aligned}
$$
令导数为 0 ，$\frac{\partial J(w)}{\partial w}$ = 0，解得：
$$
w=(X^TX)^{-1}X^TY
$$
PS：但是不是所有矩阵都有可逆矩阵，最小二乘法并不一定能求解所有 ***w*** 参数，因此引入梯度下降法

## 梯度下降法

### 梯度下降法参数更新计算公式

$$
w = w-a\frac{\partial J(w)}{\partial w}
$$

### 梯度下降法求解步骤

1. 设 a 学习率
2. 求解梯度
3. 梯度参数更新$w = w-a\frac{\partial J(w)}{\partial w}$

### 案例理解

现在假设有一个损失函数为：$J(w)=4w^2$

首先需要随机初始化 ***w*** ，假设 ***w*** = 4，同时设定学习率为 0.1。损失函数的导数如下式所示：
$$
w_0=4,a=0.1,\frac{\partial J(w)}{\partial w}=8w
$$
第一次 ***w*** 更新的过程计算如下：
$$
\begin{aligned}
w_1 &= w_0-0.1\times\frac{\partial J(w)}{\partial w}\\
	&= 4-0.1\times8\times4\\
	&= 0.8
\end{aligned}
$$
后续 ***w*** 更新的过程如下：
$$
\begin{aligned}
w_2 &= 0.8-0.1\times8\times0.8=0.16\\
w_3 &= 0.16-0.1\times8\times0.16=0.032\\
w_4 &= 0.032-0.1\times8\times0.032=0.0064\\
\end{aligned}
$$
假设此时设置学习效率为 0.5：
$$
\begin{aligned}
w_0 &= 4\\
w_1 &= 4-0.5\times8\times4=-12\\
w_2 &= -12-0.5\times8\times(-12)=32\\
w_3 &= 32-0.5\times8\times32=96\\
\end{aligned}
$$

## 逻辑回归模型

### 回归与分类的区别

在机器学习有监督学习中大致可以分为两大任务，一种是回归任务，一种是分类任务，官方说法：输入变量与输出变量均为连续变量的预测问题是回归问题，输出变量为有限个离散变量的预测问题为分类问题。

#### 回归：

通俗举例，一个人每日的运动时间、睡眠时间、工作时间、饮食等一些特征来预测一个人的体重，一个人的体重的值可以有无限个值。所以预测的结果是无限的、不确定的连续数值。这样的机器学习任务就是回归任务。

#### 分类：

如果利用一个人每日的运动时间、睡眠时间、工作时间、饮食等一些特征来判断这个人的身体状况是否健康，那么最终的判断的结果就只有两种，健康和不健康。这样的输出结果为离散值，预测的结果也是一个有限的数值来代表种类。这样的机器学习任务就是分类任务。

### 逻辑回归算法原理

sigmoid 函数的定义如下：
$$
g(z)=\frac {1}{1+e^{-z}}
$$
在 sigmoid 函数中，***e*** 为欧拉常数，自变量 ***z*** 可以取任意实数，函数的值域为 [0,1]，这就相当于将输入的自变量值映射到 0-1 之间。下面将函数的自变量转化为利用输入的数据特征来表示，以此利用 sigmoid 函数对其进行映射。
$$
z=w_0x_0+w_1x_1+w_nx_n+b\\
z=W^TX+b
$$
此时自变量就转变成输入的数据特征了，特征利用向量 ***X*** 表示。同时特征还有对应的权重参数向量 ***W*** 来表示这个数据特征的重要程度，同时还有偏置参数 b。因此，逻辑回归模型可以用如下的公式来表达:
$$
g(X)=\frac{1}{1+e^{-W^TX+b}}
$$
 因此对于一个二分类问题，此时正例和反例的函数表达式就如下所示：

预测结果为正例的表达式：
$$
p(y=1|X)=\frac{1}{1+e^{-W^TX+b}}
$$
预测结果为反例的表达式：
$$
p(y=0|X)=\frac{e^{-W^TX+b}}{1+e^{-W^TX+b}}=1-p(y=1|X)
$$
在函数的计算推导过程中，考虑到正反两例情况，因此结合上述两个例子合并起来得到如下公式：
$$
p(y|X)=p(y|X)^y[1-p(y|X)]^{1-y}
$$

### 逻辑回归——参数更新

损失函数公式为：
$$
J(w,b)=-\frac {1}{m}\sum_{i=1}^{m}[y_i\log \sigma(w^Tx_i+b)+(1-y_i)\log (1-\sigma(w^Tx_i+b))]
$$

参数 ***w*** 的偏导数的计算步骤如下所示：
$$
\begin{aligned}
\frac{\partial J(w,b)}{\partial w}
	&=-\frac{1}{m}\sum_{i=1}^{m}\frac {\partial [y_i\log \sigma(w^Tx_i+b)+(1-y_i) \log(1-\sigma(w^Tx_i+b))]}{\partial w}\\
	&=-\frac{1}{m} \sum_{i=1}^{m}y_i\frac{\sigma (w^Tx_i+b)[1-\sigma(w^Tx_i+b)]}{\sigma(w^Tx_i+b)}x_i+(1-y_i)\frac {-\sigma (w^Tx_i+b)[1-\sigma(w^Tx_i+b)]}{1-\sigma (w^Tx_i+b)}x_i\\
	&=-\frac{1}{m}\sum_{i=1}^{m}y_ix_i-\sigma(w^Tx_i+b)x_i\\
	&=\frac {1}{m}\sum_{i=1}^{m}(\sigma(w^Tx_i+b)-y_i)x_i)
\end{aligned}
$$
参数 ***b*** 的偏导数计算步骤如下所示：
$$
\begin{aligned}
\frac{\partial J(w,b)}{\partial b}
	&=-\frac{1}{m}\sum_{i=1}^{m}\frac {\partial [y_i\log \sigma(w^Tx_i+b)+(1-y_i) \log(1-\sigma(w^Tx_i+b))]}{\partial b}\\
	&=-\frac{1}{m} \sum_{i=1}^{m}y_i\frac{\sigma (w^Tx_i+b)[1-\sigma(w^Tx_i+b)]}{\sigma(w^Tx_i+b)}+(1-y_i)\frac {-\sigma (w^Tx_i+b)[1-\sigma(w^Tx_i+b)]}{1-\sigma (w^Tx_i+b)}\\
	&=-\frac{1}{m}\sum^{m}_{i=1}y_i[1-\sigma(w^Tx_i+b)]+(y_i-1)\sigma(w^Tx_i+b)\\
	&=\frac{1}{m}\sum^{m}_{i=1}\sigma(w^Tx_i+b)-y_i
\end{aligned}
$$

## 分类和回归模型评价指标

### 分类模型的评价指标

官方的评价指标定义稍显抽象，在此引入一个案例帮助自己更好的理解各个评价指标的具体含义。

#### 案例

现在有一个深度学习的模型来检测病人是否有肿瘤，显然这是一个二分类的任务，只有两种结果，第一种结果是判定有肿瘤，第二种是判定没有肿瘤。假设现在有 100 个人，其中有 10 个人是患有肿瘤的，90 个人是健康的；现在这个深度学习模型对这 100个人进行判断，最后的判断结果为有 10 个人患有肿瘤，90 个人是健康的。对比真实的结果发现，10 个人被判断患有肿瘤的人中有5人是健康的，另外 5 个人是患有肿瘤的；而被判断为健康的 90 人中有 85 个人是健康的，另外 5 个人是患有肿瘤的。

在计算评价指标之前先定义一些参数，首先本案例的目标是判定一个人是否有肿瘤，那么有肿瘤就是正类，健康就是反类。定义参数如下：

- 真实为正类，预测为正类，称为真正，用 TP 表示。本案例中一共有 10 个正类（患有肿瘤）其中有 5 个被预测出来了；因此本案例的 TP = 5。
- 真实为负类，预测为负类，称为真负，用 TN 表示。本案例中一共有 90 个负类（健康）其中有 85 个被预测出来了；因此本案例的 TN = 85。
- 真实为负类，预测为正类，称为假正（误报），用 FP 表示。本案例中一共有 90 个负类（健康），其中有 5 个被预测为正类（患有肿瘤）；因此本案例的 FP = 5。
- 真实为正类，预测为负类，称为假负(漏报)，用 FN 表示。本案例中一共有 10 个正类（患有肿瘤），其中有 5 个被预测为负类（健康）；因此本案例的 FN = 5。

#### 准确率：

通俗的说就是有多少判断对了，这个评价指标是将正类和负类是否都预测对包含在内，因此准确率的计算公式如下所示：
$$
ACC=\frac {TP+TN}{TP+TN+FP+FN}
$$
利用该公式计算模型的准确率为：$ACC=\frac{5+85}{5+85+5+5}=90\%$

从最后的准确率来看，模型的效果貌似是不错的，但是在案例中我们看到有 10 个人患有肿瘤，这个模型只预测出了 5 个，如果我们偏信这个模型的话，那将会有 5 个人漏报，产生的结果将十分严重。导致这样的结果的原因是因是样本不均衡，因为正类和负类数量相差太大。假设在这样一个不均衡的样本中，直接认定所有的样本都是正类，那么准确率也能达到 90%。因此此时准确率就不能很好的作为模型的评价指标了。

#### 精确率：

精确率又叫查准率，精确率告诉我们在预测出来的正类样本中，真正为正类的比例是多少，精确率的计算公式为：
$$
P=\frac{TP}{TP+FP}
$$
利用该公式计算模型的精确率为：$P=\frac{5}{5+5}=50\%$，由此可见该模型对患有癌症的识别并不准确。

#### 召回率：

召回率又叫查全率，召回率告诉我们所有的正类样本中，有多少被识别出来了，具体计算公式如下：
$$
Recall=\frac{TP}{TP+FN}
$$
利用该公式计算模型的召回率为：$Recall=\frac{5}{5+5}=50\%$

#### F~1~ 值：

F~1~ 值是精确率和召回率的调和均值，具体计算公式如下所示：
$$
F_1=\frac {2TP}{2TP+FP+FN}
$$
精确率和准确率都高的情况下，F~1~的值也会高。

### 回归模型的评价指标

和分类模型不同的是，回归模型的输出是一个连续的值，或者可以说是不确定的值。那么用精确度和准确度等评价指标就不那么合理了。从回归模型的任务性质来说，我们希望得到的回归预测值尽可能的接近于真实值，也就是预测值和真实值之间的误差尽可能小，因此我们可以用如下的评价指标。

#### 平均绝对误差(MAE)

$$
MAE=\frac{1}{n}\sum_{i=1}^{n}\lvert(y_i-\hat y_i)\rvert
$$

其中 y~i~ 为真实值，$\hat y_i$ 为回归预测值，n 为回归的数据个数。

#### 均方误差(MSE)

$$
MSE=\frac{1}{n}\sum^{n}_{i=1}(y_i-\hat y_i)^2
$$

其中 y~i~ 为真实值，$\hat y_i$ 为回归预测值，n 为回归的数据个数。该公式也用于回归的损失函数，并且可导（MAE绝对值不是处处可导），即最小化均方误差。值越小，性能越好。

#### 均方根误差(RMSE)

$$
RMSE=\sqrt{\frac{1}{n}\sum^{n}_{i=1}(y_i-\hat y_i)^2}
$$

其中 y~i~ 为真实值，$\hat y_i$ 为回归预测值，n 为回归的数据个数。实质与均方根误差 MSE 相同，主要用于降低均方误差的数量级，防止均方误差 MSE 看起来很大。

#### MAPE

$$
MAPE=\frac{1}{n}\sum_{i=1}^{n}\lvert \frac{y_i-\hat y_i}{y_i}\rvert\times100\%
$$

其中 y~i~ 为真实值，$\hat y_i$ 为回归预测值，n 为回归的数据个数。注意由于这里用了 y~i~ 作为分母，所以当测量真实值有数据为 0 时，即存在分母为 0 的情况，该指标公式就不可用了，值越小越好。